#  Deep Neural Network
class DNN(nn.Module):
    def __init__(self, layers):
        super().__init__()  # call __init__ from parent class

        self.layers = layers
        'activation function'
        self.activation = nn.Tanh()

        'Initialize neural network as a list using nn.Modulelist'
        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])

        'Xavier Normal Initialization'
        for i in range(len(self.layers) - 1):
            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)

            # set biases to zero
            nn.init.zeros_(self.linears[i].bias.data)

    'foward pass'

    def forward(self, x):

        if torch.is_tensor(x) != True:
            x = torch.from_numpy(x)
        #
        # u_b = torch.from_numpy(ub).float().to(device)
        # l_b = torch.from_numpy(lb).float().to(device)
        #
        # # preprocessing input
        # x = (x - l_b) / (u_b - l_b)  # feature scaling

        # convert to float
        a = x.float()

        for i in range(len(self.layers) - 2):
            z = self.linears[i](a)

            a = self.activation(z)

        a = self.linears[-1](a)

        return a
